%TODO: Add background content

\chapter{Background}\label{c:background}

Several program analysis techniques have been proposed in the literature, in
order to aid developers and users discover interesting program properties within
their software. For example, one might be interested in finding out whether
there are memory leaks or whether some piece of code is reachable.

Such techniques come in different flavors, either static or dynamic, and are
usually based on strong mathematical concepts. In this section we provide 
a brief background on these concepts and the frameworks utilized towards 
the development of program analysis tools.

\section{Static Program Analysis}

Static program analysis is a program analysis technique that aims to reason
about a program's behaviors without actually executing it. It has been
heavily utilized by optimizing compilers since their early stages and it
has also found several other applications, among the areas of softare security,
software correctness\cite{spa}. The question of whether 
a program is correct or may terminate for all possible inputs is in general
undecidable. However, static analysis techniques are able to tackle
undecidability by overapproximating or underapproximating the initial problem,
attempting to reason over a simplified version of it.

%
% examples of analyses 
%

There is a plethora of different static analysis algorithms in the literature, each one
met in several domains. For instance, one may utilize analyses such as \emph{liveness} and/or 
\emph{pointer} analyses in an optimizing compiler with the intention of eliminating
dead code regions or performing a constant propagation/folding optimization.
Similarly, a \emph{reachability} analysis that determines whether a specific program point is 
reachable could be used by a software correctness tool to make sure that
an erroneous state is actually never reached.

There is a variety of design choices that may prove essential towards the \emph{scalability}
and \emph{precision} of a static program analysis algorithm. We briefly describe some of
those choices in the context of this work.

\subsection{Whole-Program vs. Modular Analyses}

Whole-Program analyses need not be confused with \emph{inter-procedural} analyses. 
An inter-procedural analysis considers multiple functions in order to perform
its computations. However, a \emph{whole}-program analysis does not only reason
about a program's properties across different functions, but also includes any 
external dependencies during its computations, such as external libraries or 
native calls.

On the contrary, a \emph{modular}-analysis reasons about specific parts of a
program, disregarding dependencies across different calls and external code.
Modular-analysis is fundamentally proportional to \emph{intra}-procedural analyses.
In the intra-procedural setting, an analysis would restrict its reasoning within
a specific function bound, overlooking the way that function calls or external
dependencies affect the computations inside the function. However, someone could
also perform a modular-style analysis across a relatively small set of functions,
that is an \emph{inter-procedural} analysis.

Whether the analysis would reason about a program's properties in a whole-program
or a modular manner is of great importance, because such a decision affects the
way that the analysis practitioner would tune its performance - scalability and
precision. In the whole-program analysis setting the design tradeoffs between
scalability and precision might prove crucial towards the overall analysis
reasoning, though a modular-analysis may be able to perform more precise reasoning,
due to the relatively restricted area of interest. Whole-program analyses are
commonly used in the analysis of languages with complex language features. Those
languages, such as the object-oriented ones need to reason about language constructs
that may reside on the heap, and thus they may benefit from a whole-program analysis
reasoning. Such an analysis may not take into consideration the
exact ordering of the instructions of a program, sacrificing precision towards
better scalability. 

\subsection{Flow-Sensitivity}

The concept of whether an analysis is designed with respect to 
the instruction ordering is called \emph{flow-sensitivity}.
On the contrary, an analysis that is not designed that way is 
called flow-insensitive. Flow-sensitivity is tightly related with
the scalability and precision of static program analyses. 
Whole-program analyses may usually omit the control-flow constructs
during any of their reasoning. However, many whole-program static
analyses like the \doop{} framework's Pointer analysis for Java
manage to add flow-sensitivity to their core analysis by slight
preprocessing\cite{points-to15}.

Several tools utilize state-of-the-art compiler technologies to 
convert the input source to a lower level \emph{intermediate-representation (IR)}.
For instance, there is a plethora of tools that utilize the \emph{LLVM Compiler 
Infrastructure}\cite{llvm-lattner} for \emph{C}-like languages,
or \emph{Soot - A Java optimization framework}\cite{soot} - for JVM languages. 
These tools may further lower the source code in a 
\emph{Static Single Assignment (SSA)} form. In SSA form, each variable is 
assigned exactly once, never to be re-assigned again and it is also defined 
before any of its uses. 

\subsection{Path-Sensitivity}

%TODO: Stuff about soundness/completeness of static program analyses

\section{Symbolic Execution}
Foo

\section{Theorem Provers}
Lalal

\section{Datalog}
Datalog stuff
